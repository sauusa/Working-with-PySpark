{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Channel Management - Take home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Create Spark session\n",
    "\n",
    "##### Using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL for Retail Management\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read from the input file\n",
    "\n",
    "#### Note: \n",
    "          For this assignment input file name is changed to \"channel_dataset.csv\"\n",
    "          This file is kept in the local directory called 'input', please changed the filename & path before use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"input/channel_dataset.csv\"\n",
    "input_retail = spark.read.csv(path,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking schema and sample records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- flyer_id: string (nullable = true)\n",
      " |-- merchant_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#user_log.take(1)\n",
    "input_retail.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a Spark Table from file\n",
    "\n",
    "#### Note: \n",
    "          Table Name: input_retail_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_retail.createOrReplaceTempView(\"input_retail_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+--------+-----------+\n",
      "|           timestamp|             user_id|             event|flyer_id|merchant_id|\n",
      "+--------------------+--------------------+------------------+--------+-----------+\n",
      "|2018-10-01T13:54:...|9ea672779feb1e088...|shopping_list_open|    null|       null|\n",
      "|2018-10-01T13:34:...|01ca5536abc5e0992...|shopping_list_open|    null|       null|\n",
      "+--------------------+--------------------+------------------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM input_retail_table LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write SQL queries to extract the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Date from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------------------+--------------------+----------+--------+-----------+\n",
      "|           timestamp|to_date(input_retail_table.`timestamp`)|             user_id|     event|flyer_id|merchant_id|\n",
      "+--------------------+---------------------------------------+--------------------+----------+--------+-----------+\n",
      "|2018-10-01T13:56:...|                             2018-10-01|1c1231e7a41a1bee1...|flyer_open| 2016315|       2268|\n",
      "|2018-10-01T13:34:...|                             2018-10-01|7c3e5dadd6c0d7170...|flyer_open| 1993325|       2188|\n",
      "|2018-10-01T13:31:...|                             2018-10-01|9ea672779feb1e088...|flyer_open| 2002542|       3383|\n",
      "|2018-10-01T13:39:...|                             2018-10-01|4997a2ca5d6f3a8e2...|flyer_open| 2031695|       2148|\n",
      "|2018-10-01T13:31:...|                             2018-10-01|71e01a216fd6d973c...|flyer_open| 1996644|       2694|\n",
      "|2018-10-01T13:31:...|                             2018-10-01|87c2f342e40511b1e...|flyer_open| 1994962|        221|\n",
      "|2018-10-01T13:48:...|                             2018-10-01|b38f97880ccb69e8f...|flyer_open| 1994508|        986|\n",
      "|2018-10-01T13:40:...|                             2018-10-01|b38f97880ccb69e8f...|flyer_open| 1985422|       2516|\n",
      "|2018-10-01T13:31:...|                             2018-10-01|71e01a216fd6d973c...|flyer_open| 2000528|       2598|\n",
      "|2018-10-01T13:31:...|                             2018-10-01|89449f6296fecb861...|flyer_open| 1994963|        221|\n",
      "+--------------------+---------------------------------------+--------------------+----------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.udf.register(\"get_hour\", lambda x: int(datetime.datetime.fromtimestamp(x / 1000.0).hour))\n",
    "\n",
    "spark.sql('''\n",
    "        SELECT timestamp, to_date(timestamp), user_id, event, flyer_id, merchant_id \n",
    "        from input_retail_table where flyer_id is NOT NULL\n",
    "''').show(10,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing around the table for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+----------+--------+-----------+\n",
      "|timestamp                |event     |flyer_id|merchant_id|\n",
      "+-------------------------+----------+--------+-----------+\n",
      "|2018-10-01T19:45:44-04:00|flyer_open|2015666 |1344       |\n",
      "|2018-10-01T16:29:25-04:00|flyer_open|2026399 |2549       |\n",
      "|2018-10-01T16:19:19-04:00|flyer_open|2026386 |5265       |\n",
      "|2018-10-01T16:16:59-04:00|flyer_open|1982865 |246        |\n",
      "|2018-10-01T13:56:36-04:00|flyer_open|2016315 |2268       |\n",
      "|2018-10-01T11:39:36-04:00|flyer_open|2018389 |2123       |\n",
      "|2018-10-01T11:24:14-04:00|flyer_open|1983425 |2366       |\n",
      "|2018-10-01T08:53:11-04:00|flyer_open|1990059 |249        |\n",
      "|2018-10-01T08:49:24-04:00|flyer_open|2009425 |2631       |\n",
      "|2018-10-01T08:45:56-04:00|flyer_open|1990059 |249        |\n",
      "|2018-10-01T06:22:48-04:00|flyer_open|2016095 |2694       |\n",
      "|2018-10-01T06:19:52-04:00|flyer_open|2016095 |2694       |\n",
      "+-------------------------+----------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "spark.sql('''\n",
    "    select flyer_id, count(*) from input_retail_table\n",
    "       where flyer_id IS NOT NULL\n",
    "       group by flyer_id\n",
    "''').show(10)\n",
    "\n",
    "spark.sql('''\n",
    "    select distinct(event) from input_retail_table       \n",
    "''').show(10,False)\n",
    "\"\"\"\n",
    "spark.sql('''\n",
    "    select timestamp, event, flyer_id, merchant_id from input_retail_table as lrt\n",
    "    where lrt.user_id = '1c1231e7a41a1bee17dd8e8111ebaef941525995f330199844fd9a0293edf9aa'\n",
    "    and lrt.event = 'flyer_open'\n",
    "    order by timestamp desc\n",
    "    ''').show(100,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the desired output for 1 record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------+---------------------------+-----------------+\n",
      "|maxT                     |minT                     |time_diff                  |time_diff_in_Sec |\n",
      "+-------------------------+-------------------------+---------------------------+-----------------+\n",
      "|2018-10-01T12:03:52-04:00|2018-10-01T11:30:48-04:00|1 minutes 22.666667 seconds|82.66666666666667|\n",
      "+-------------------------+-------------------------+---------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using lambda function timestamp_diff:\n",
    "#          timestamp_diff(to_timestamp(min(timestamp)),to_timestamp(max(timestamp))) as time_diff_in_second\n",
    "#def timestamp_diff(time1: datetime.datetime, time2: datetime.datetime):\n",
    "#    return int((time1-time2).total_seconds()*1000)\n",
    "#\n",
    "#spark.udf.register(\"timestamp_diff\", timestamp_diff)\n",
    "\n",
    "\"\"\"\n",
    "spark.sql('''\n",
    "    select timestamp, event, flyer_id, merchant_id from input_retail_table as lrt\n",
    "    where lrt.user_id = '0017345b89958a1d8cae79020dbbf6e2f687124ae8bf937fa6ed729e66a13f91'\n",
    "    and lrt.event = 'flyer_open'\n",
    "    order by timestamp desc\n",
    "    ''').show(100,False)\n",
    "    \"\"\"\n",
    "df = spark.sql(\"\"\"\n",
    "    select max(timestamp) as maxT, min(timestamp) as minT, \n",
    "         ((( to_timestamp(max(timestamp)) - to_timestamp(min(timestamp)) ) / count(timestamp))) as time_diff,\n",
    "         ((( unix_timestamp(to_timestamp(max(timestamp))) - unix_timestamp(to_timestamp(min(timestamp))) ) \n",
    "                 / count(timestamp))) as time_diff_in_Sec\n",
    "           from input_retail_table as lrt\n",
    "           where lrt.user_id = '0017345b89958a1d8cae79020dbbf6e2f687124ae8bf937fa6ed729e66a13f91'\n",
    "             and (lrt.event = 'flyer_open' or lrt.event = 'item_open')\n",
    "    \"\"\")\n",
    "\n",
    "df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Not  Needed\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col\\nfrom pyspark.sql.functions import to_timestamp, current_timestamp\\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\\n\\n#time1 = datetime.datetime.now()\\n#time2 = datetime.datetime.strptime(\"2020-11-09 13:54:19.541035\",\"%Y-%m-%d %H:%M:%S.%f\")\\n#timediff = time2 - time1\\n#print(time1, time2, datetime.timedelta(time2 - time1).total_seconds() )\\n\\ndf.withColumn(\"max_timestamp\", to_timestamp(col(\"maxT\")))   .withColumn(\"min_timestamp\", to_timestamp(col(\"minT\")))   .withColumn(\"DiffInSeconds\", col(\"time_diff\").cast(LongType))   .show(false)\\n  '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Not  Needed\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "#time1 = datetime.datetime.now()\n",
    "#time2 = datetime.datetime.strptime(\"2020-11-09 13:54:19.541035\",\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "#timediff = time2 - time1\n",
    "#print(time1, time2, datetime.timedelta(time2 - time1).total_seconds() )\n",
    "\n",
    "df.withColumn(\"max_timestamp\", to_timestamp(col(\"maxT\"))) \\\n",
    "  .withColumn(\"min_timestamp\", to_timestamp(col(\"minT\"))) \\\n",
    "  .withColumn(\"DiffInSeconds\", col(\"time_diff\").cast(LongType)) \\\n",
    "  .show(false)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Store the output result in csv file to be used for BI\n",
    "\n",
    "Output file: result_dataset.csv \n",
    "##### Note:\n",
    "          Change the output file name and path before running, for test purpose local path is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+--------------------+---------------+\n",
      "|             user_id|merchant_id|     tdate|            avg_time|avg_time_in_Sec|\n",
      "+--------------------+-----------+----------+--------------------+---------------+\n",
      "|00f4a877b16a98a20...|       2345|2018-10-01|1 hours 37 minute...|         5861.5|\n",
      "|031eb2968048a6b02...|       2609|2018-10-01|           0 seconds|            0.0|\n",
      "|07ee047fa6b71db13...|       2268|2018-10-01|1 minutes 52 seconds|          112.0|\n",
      "|0cdc3cd22bb3acce6...|       2163|2018-10-01|           0 seconds|            0.0|\n",
      "|15e3005b2d8fe8c74...|       2046|2018-10-01|           0 seconds|            0.0|\n",
      "|1679d463324e9c02b...|       2366|2018-10-01|4 hours 24 minute...|        15870.0|\n",
      "|24c0f98368d52f3a8...|       2944|2018-10-01|           0 seconds|            0.0|\n",
      "|3378aa355a65ccc88...|       1991|2018-10-01|           0 seconds|            0.0|\n",
      "|365a011424432621d...|       2345|2018-10-01|           0 seconds|            0.0|\n",
      "|3d702d209609ae48c...|       2453|2018-10-01|           0 seconds|            0.0|\n",
      "|3dfefb729dd4810d2...|       2085|2018-10-01| 9 minutes 4 seconds|          544.0|\n",
      "|40e420f188d341cfd...|       2386|2018-10-01|           0 seconds|            0.0|\n",
      "|44c0ed6089afc9d34...|       2066|2018-10-01|           0 seconds|            0.0|\n",
      "|44e40d91f3f8b39f1...|       2349|2018-10-01|           0 seconds|            0.0|\n",
      "|491d3a5cb0f447f31...|        232|2018-10-01|           0 seconds|            0.0|\n",
      "|650150ff2fe45d7de...|       2681|2018-10-01|           0 seconds|            0.0|\n",
      "|6a278d34c73d264ba...|       2633|2018-10-01|           0 seconds|            0.0|\n",
      "|6a278d34c73d264ba...|        499|2018-10-01|           0 seconds|            0.0|\n",
      "|6a5b9fb789bc4ef0c...|       2610|2018-10-01|           0 seconds|            0.0|\n",
      "|6bffbbfc7bfcae5e2...|        221|2018-10-01|           0 seconds|            0.0|\n",
      "|7a8e0c9da198174eb...|       2031|2018-10-01|           0 seconds|            0.0|\n",
      "|87d4427f66eba5653...|       2484|2018-10-01|           0 seconds|            0.0|\n",
      "|898d4d2f9fda029bf...|       2139|2018-10-01|           0 seconds|            0.0|\n",
      "|8d08781fb014efd11...|       2480|2018-10-01|           0 seconds|            0.0|\n",
      "|8df6c297c3d0a10b0...|       2638|2018-10-01|           0 seconds|            0.0|\n",
      "|8ee06d91bd22e43cc...|       2268|2018-10-01|           0 seconds|            0.0|\n",
      "|8f6dd2335b1c4a451...|        986|2018-10-01|           0 seconds|            0.0|\n",
      "|8f78dcbd5137335c5...|        446|2018-10-01|           0 seconds|            0.0|\n",
      "|90f0ea307663e1cb7...|       2621|2018-10-01|           0 seconds|            0.0|\n",
      "|92e0edd3590a2b45c...|       2284|2018-10-01|1 hours 8 minutes...|         4129.5|\n",
      "|958ab0f7ace73272f...|        819|2018-10-01|8 minutes 47.5 se...|          527.5|\n",
      "|95eb8ebfcfb467736...|       2430|2018-10-01|           0 seconds|            0.0|\n",
      "|9830c10249b449e59...|       2628|2018-10-01|           0 seconds|            0.0|\n",
      "|9acd0e1e4af6d022e...|       2066|2018-10-01|           0 seconds|            0.0|\n",
      "|9cb606b9bdfd0c46b...|       2284|2018-10-01|           0 seconds|            0.0|\n",
      "|a201f01c1647f2e0e...|       2484|2018-10-01|           0 seconds|            0.0|\n",
      "|a44d807bfa2973d5a...|        246|2018-10-01|1 hours 16 minute...|         4614.0|\n",
      "|a83408f90abc0b90a...|        250|2018-10-01|           0 seconds|            0.0|\n",
      "|acde3a538303dc35a...|       2123|2018-10-01|           0 seconds|            0.0|\n",
      "|b0b042de81771f24e...|       2300|2018-10-01|           0 seconds|            0.0|\n",
      "|b38f97880ccb69e8f...|       1145|2018-10-01|12 minutes 33 sec...|          753.0|\n",
      "|b38f97880ccb69e8f...|       3101|2018-10-01|1 hours 39 minute...|         5946.2|\n",
      "|b4358b0a129772448...|       2114|2018-10-01|           0 seconds|            0.0|\n",
      "|b60e6c8c693389166...|       2484|2018-10-01|           0 seconds|            0.0|\n",
      "|b6782014015dc24ee...|       2365|2018-10-01|           0 seconds|            0.0|\n",
      "|bdffe32d31b768192...|       2280|2018-10-01|           0 seconds|            0.0|\n",
      "|c8c436d08fadc07e3...|       2282|2018-10-01|           0 seconds|            0.0|\n",
      "|ce08b6995a1d72409...|        563|2018-10-01|           0 seconds|            0.0|\n",
      "|ce0dedb2415b59c38...|       2280|2018-10-01|           0 seconds|            0.0|\n",
      "|ce4c999b047a2abf2...|       2446|2018-10-01|           0 seconds|            0.0|\n",
      "+--------------------+-----------+----------+--------------------+---------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    select user_id, merchant_id, to_date(timestamp) as tdate, \n",
    "         ( (to_timestamp(max(timestamp)) - to_timestamp(min(timestamp))) / count(timestamp)) as avg_time,\n",
    "         ( (( unix_timestamp(to_timestamp(max(timestamp))) - unix_timestamp(to_timestamp(min(timestamp))) ) \n",
    "                 / count(timestamp))) as avg_time_in_Sec\n",
    "      from input_retail_table as lrt\n",
    "      where (lrt.event = 'flyer_open' or lrt.event = 'item_open')\n",
    "      group by user_id, merchant_id, tdate\n",
    "    ''').show(50,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the output path name before running\n",
    "out_path = \"C:/Users/saurabh/input/result/result_dataset.csv\"\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "    select user_id, merchant_id, to_date(timestamp) as tdate, \n",
    "         string( (to_timestamp(max(timestamp)) - to_timestamp(min(timestamp))) / count(timestamp)) as avg_time,\n",
    "               ( (( unix_timestamp(to_timestamp(max(timestamp))) - unix_timestamp(to_timestamp(min(timestamp))) ) \n",
    "                 / count(timestamp))) as avg_time_in_Sec\n",
    "      from input_retail_table as lrt\n",
    "      where (lrt.event = 'flyer_open' or lrt.event = 'item_open')\n",
    "      group by user_id, merchant_id, tdate\n",
    "    \"\"\")\n",
    "\n",
    "#df.show()\n",
    "\n",
    "# In case of large data set, where partition is required, we use 1 of the following:\n",
    "\n",
    "#df.coalesce(1).write.csv(out_path)\n",
    "#df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(out_path)\n",
    "\n",
    "#df.write.format(\"com.databricks.spark.csv\").option(\"header\",\"false\").mode(\"overwrite\").save(out_path)\n",
    "#df.write.format(\"com.databricks.spark.csv\").option(\"header\",\"true\").save(out_path)\n",
    "\n",
    "# For smaller dataset, we can use the following\n",
    "df.toPandas().to_csv(out_path, sep=',', header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
